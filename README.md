# Taller: Uso practico de Airflow

## ğŸ“Œ DescripciÃ³n

**Este repositorio contiene un entorno para el procesamiento de datos y entrenamiento de modelos basado en `Apache Airflow`, `Docker` y `Python`. Su diseÃ±o modular permite la automatizaciÃ³n de flujos de trabajo con soporte para `PostgreSQL`, `MySQL` y `Redis`, mejorando la gestiÃ³n y orquestaciÃ³n de tareas en entornos distribuidos.

## ğŸ“‚ Estructura del Proyecto

La estructura del proyecto estÃ¡ organizada para garantizar una correcta separaciÃ³n de responsabilidades y modularidad. Los flujos de trabajo se encuentran en la carpeta `dags/`, donde cada script maneja tareas especÃ­ficas como la carga, eliminaciÃ³n y entrenamiento de datos. La carpeta `datos/` contiene los archivos de entrada requeridos para el procesamiento, mientras que `logs/` almacena los registros de ejecuciÃ³n para facilitar el monitoreo. Los archivos de configuraciÃ³n, como `docker-compose.yml` y `Dockerfile`, permiten la implementaciÃ³n del entorno en contenedores, asegurando escalabilidad y reproducibilidad.

```
TALLER_3/
â”‚â”€â”€ dags/                      # DefiniciÃ³n de flujos de trabajo
â”‚   â”œâ”€â”€ carga_datos.py         # Carga de datos
â”‚   â”œâ”€â”€ elimina_datos.py       # EliminaciÃ³n de datos
â”‚   â”œâ”€â”€ train.py               # Entrenamiento del modelo
â”‚â”€â”€ datos/                     # Almacenamiento de datos
â”‚   â”œâ”€â”€ penguins_lter.csv      # Conjunto de datos en formato CSV
â”‚â”€â”€ logs/                      # Registro de ejecuciÃ³n
â”‚â”€â”€ plugins/                   # Extensiones para Airflow
â”‚â”€â”€ docker-compose.yml         # OrquestaciÃ³n de servicios
â”‚â”€â”€ Dockerfile                 # ConstrucciÃ³n de la imagen Docker
â”‚â”€â”€ requirements.txt           # Dependencias del proyecto
```

## ğŸ›  Instancia MySQL
Para crear la instancia de la base de datos mysql, fue necesario incluir el servicio mysql dentro del docker-compose y el volume mysql_data como se encuentra a continuaciÃ³n:

```
mysql:
    # Nombre del servicio
    image: mysql:latest  # Imagen Docker para el contenedor MySQL
    ports:
      - "3306:3306"  # Mapeo de puertos del contenedor al host
    environment:
      MYSQL_ROOT_PASSWORD: airflow  # ContraseÃ±a de root para MySQL
      MYSQL_DATABASE: penguin_data  # Nombre de la base de datos MySQL
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]  # Comando de verificaciÃ³n de salud
      interval: 10s  # Frecuencia de la verificaciÃ³n de salud
      timeout: 5s  # Tiempo mÃ¡ximo de espera para la verificaciÃ³n de salud
      retries: 3  # NÃºmero de intentos de verificaciÃ³n de salud
      start_period: 10s  # Tiempo antes de iniciar las verificaciones de salud
    restart: always  # PolÃ­tica de reinicio del contenedor
```

## âš™ï¸ Requisitos

Para ejecutar este repositorio correctamente, es necesario contar con herramientas y configuraciones especÃ­ficas. `Docker` y `Docker Compose` permiten la ejecuciÃ³n de los servicios en contenedores, asegurando un entorno reproducible y estable. `Python 3.x` es el lenguaje base para los scripts de procesamiento de datos. AdemÃ¡s, las librerÃ­as listadas en `requirements.txt`, como `pandas`, `scikit-learn` e `imbalanced-learn`, proporcionan las funcionalidades necesarias para el anÃ¡lisis de datos y entrenamiento de modelos. Finalmente, `Apache Airflow` gestiona la ejecuciÃ³n de tareas automatizadas mediante flujos de trabajo definidos en DAGs.

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

La instalaciÃ³n y configuraciÃ³n del proyecto se realiza en pocos pasos, asegurando una rÃ¡pida implementaciÃ³n del entorno. Primero, se clona el repositorio para obtener el cÃ³digo fuente y se accede al directorio del proyecto. Luego, se instalan las dependencias listadas en `requirements.txt`, garantizando que todas las herramientas necesarias estÃ©n disponibles. Posteriormente, se inician los servicios con `docker-compose`, lo que levanta los contenedores de Airflow, PostgreSQL, MySQL y Redis de manera automatizada. Finalmente, la interfaz web de Airflow puede ser accedida a travÃ©s del navegador para gestionar y ejecutar los flujos de trabajo definidos.

1. Clonar el repositorio
```
git clone https://github.com/JohnSanchez27/MLOps_Taller3.git
cd MLOps_Taller3

```

2. Construir el contenedor
```
docker-compose up --build
```

3. Para detener los contenedores sin eliminarlos
```
docker-compose stop
```
4. Acceder a la interfaz de Airflow 
```
 https://localhost:8080 
```

## ğŸ—ï¸ Arquitectura y ConfiguraciÃ³n de Servicios

La arquitectura del proyecto estÃ¡ diseÃ±ada para garantizar la ejecuciÃ³n eficiente y escalable de flujos de trabajo en entornos distribuidos. Airflow gestiona la planificaciÃ³n y ejecuciÃ³n de tareas mediante `CeleryExecutor`, que permite distribuir la carga entre mÃºltiples workers. Las bases de datos `PostgreSQL` y `MySQL` almacenan la metadata de Airflow y los datos procesados respectivamente. Redis actÃºa como un `broker` de mensajes, facilitando la comunicaciÃ³n entre los diferentes componentes del sistema. El uso de volÃºmenes persistentes asegura la integridad de los datos, evitando pÃ©rdidas tras reinicios del sistema.

## ğŸ”¥ Retos del Uso de Airflow
El uso de Airflow en entornos productivos conlleva varios desafÃ­os tÃ©cnicos que deben ser considerados para su correcta implementaciÃ³n. La configuraciÃ³n inicial requiere definir correctamente las variables de entorno y la conexiÃ³n entre servicios. La gestiÃ³n de dependencias debe ser precisa para evitar incompatibilidades con la versiÃ³n de Airflow utilizada. El monitoreo y depuraciÃ³n de errores es esencial debido a la generaciÃ³n de mÃºltiples registros de ejecuciÃ³n. Para manejar grandes volÃºmenes de datos, es necesario optimizar la configuraciÃ³n de `CeleryExecutor` y los recursos asignados a `Redis`, `PostgreSQL` y `MySQL`. AdemÃ¡s, la estructuraciÃ³n adecuada de los DAGs permite optimizar la ejecuciÃ³n y reducir tiempos de procesamiento.

## ğŸ“Š Uso
El proyecto se gestiona a travÃ©s de la interfaz web de Airflow, donde se pueden visualizar, programar y ejecutar flujos de trabajo. Los DAGs ubicados en `dags/` pueden ser modificados para adaptarse a nuevos requerimientos o incorporar nuevas funcionalidades. Los registros de ejecuciÃ³n almacenados en `logs/` proporcionan informaciÃ³n detallada sobre el estado de cada tarea, lo que facilita la depuraciÃ³n y optimizaciÃ³n del sistema.
